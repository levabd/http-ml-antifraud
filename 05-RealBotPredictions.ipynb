{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/levabd/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['plt']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 10)\n",
    "pd.set_option('display.max_columns', 1100)\n",
    "\n",
    "import os\n",
    "\n",
    "%pylab inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train part\n",
    "\n",
    "### Load data from logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from lib.parsers.logParser import LogParser\n",
    "\n",
    "l_parser = LogParser(log_folder='Logs/')\n",
    "\n",
    "main_data, values_data, order_data = l_parser.parse_train_sample(0, 10)\n",
    "\n",
    "list_ua = pd.DataFrame(main_data).User_Agent.value_counts().index.tolist()\n",
    "\n",
    "# For NaN Useragent\n",
    "list_ua.append('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289269/289269 [00:03<00:00, 85643.96it/s]\n",
      " 21%|██        | 60870/289269 [00:00<00:00, 608694.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse dummy orders shape: \n",
      "(289269, 2277)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289269/289269 [00:00<00:00, 596599.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse dummy values shape: \n",
      "(289269, 361)\n"
     ]
    }
   ],
   "source": [
    "important_keys_set = {'Accept', 'Accept-Charset', 'Accept-Encoding'}\n",
    "\n",
    "orders_vectorizer = sklearn.feature_extraction.DictVectorizer(sparse=True, dtype=float)\n",
    "values_vectorizer = sklearn.feature_extraction.DictVectorizer(sparse=True, dtype=float)\n",
    "\n",
    "full_sparce_dummy = l_parser.prepare_data(orders_vectorizer, values_vectorizer, important_keys_set, fit_dict=True)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "lb = preprocessing.LabelBinarizer(sparse_output=True)\n",
    "lb.fit(list_ua)\n",
    "y = lb.transform(pd.DataFrame(main_data).User_Agent.fillna('0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14h 11min 36s, sys: 1min 5s, total: 14h 12min 41s\n",
      "Wall time: 14h 16min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "smart_clf = OneVsRestClassifier(LogisticRegression(C=100))\n",
    "smart_clf.fit(full_sparce_dummy, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277520394"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "filename = 'cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl'\n",
    "_ = joblib.dump(smart_clf, filename, compress=9)\n",
    "\n",
    "print(\"Model saved with size(Bytes): {}\".format(os.stat(filename).st_size))\n",
    "\n",
    "from lib.helpers.fileSplitter import split_file\n",
    "\n",
    "files_count = split_file(filename, 'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl')\n",
    "\n",
    "print('Splitted in {} files'.format(files_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test part\n",
    "\n",
    "### Prepare data (50/50 bots and human mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start parsing logs for distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.32it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start parsing logs for values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.28it/s]\n",
      "  0%|          | 22/877616 [00:00<1:07:46, 215.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bots Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 877616/877616 [47:34<00:00, 307.50it/s]\n",
      "100%|██████████| 1755232/1755232 [00:19<00:00, 91315.97it/s]\n",
      "  4%|▎         | 63468/1755232 [00:00<00:02, 634676.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse dummy orders shape: \n",
      "(1755232, 2277)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1755232/1755232 [00:02<00:00, 687462.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse dummy values shape: \n",
      "(1755232, 361)\n"
     ]
    }
   ],
   "source": [
    "main_bot_data, values_bot_data, order_bot_data = l_parser.parse_bot_sample(10, 20, 20, 30)\n",
    "\n",
    "main_human_frame = pd.DataFrame(main_data)\n",
    "main_human_frame['is_human'] = True\n",
    "\n",
    "main_bot_frame = pd.DataFrame(main_bot_data)\n",
    "main_bot_frame['is_human'] = False\n",
    "\n",
    "main_all = pd.concat([main_human_frame, main_bot_frame])\n",
    "\n",
    "values_all = values_data + values_bot_data\n",
    "order_all = order_data + order_bot_data\n",
    "\n",
    "list_all_ua = main_all.User_Agent.value_counts().index.tolist()\n",
    "\n",
    "# For NaN Useragent\n",
    "list_all_ua.append('0')\n",
    "\n",
    "l_parser.reassign_orders_values(order_all, values_all)\n",
    "\n",
    "test_sparce_dummy = l_parser.prepare_data(orders_vectorizer, values_vectorizer, important_keys_set, fit_dict=False)\n",
    "\n",
    "lb.fit(list_all_ua)\n",
    "y_test = lb.transform(pd.DataFrame(main_all).User_Agent.fillna('0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load classifyer from file if needed\n",
    "\n",
    "Use only `dummyordr_and_3values_fulluacls.joblib.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiles = [\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.0',\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.1',\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.2',\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.3',\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.4',\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.5'\n",
    "]\n",
    "\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "from lib.helpers.fileSplitter import cat_files\n",
    "\n",
    "cat_files(infiles, 'cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl')\n",
    "\n",
    "filename = 'cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl'\n",
    "smart_clf = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.thresholdPredictions import ThresholdPredictions\n",
    "\n",
    "pred = ThresholdPredictions(user_agent_list=list_ua, clf=smart_clf)\n",
    "y_test_names, y_predicted, compare_answers, is_bot, answers_count = pred.bot_predict(lb, test_sparce_dummy, y_test, 0.024072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_frame = pd.concat(\n",
    "    [\n",
    "        y_test_names,\n",
    "        y_predicted, \n",
    "        pd.DataFrame(compare_answers), \n",
    "        main_all.is_human,\n",
    "        pd.DataFrame(is_bot), \n",
    "        pd.DataFrame(answers_count)\n",
    "    ], keys=['test', 'predicted', 'correctness', 'is_bot_real', 'is_bot_predicted', 'count'], axis=1, join='inner')\n",
    "\n",
    "compare_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "headers_cm = confusion_matrix(compare_frame['is_bot_real'], compare_frame['is_bot_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers_FP = headers_cm.sum(axis=0) - np.diag(headers_cm)  \n",
    "headers_FN = headers_cm.sum(axis=1) - np.diag(headers_cm)\n",
    "headers_TP = np.diag(headers_cm)\n",
    "headers_TN = headers_cm.values.sum() - (headers_FP + headers_FN + headers_TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('TP: {}'.format(headers_TP))\n",
    "print('TN: {}'.format(headers_TN))\n",
    "print(\"FP: {}\".format(headers_FP))\n",
    "print(\"FN: {}\".format(headers_FN))\n",
    "print(\"Accuracy (ACC): {}\".format((headers_TP + headers_TN) / (headers_TP + headers_TN + headers_FP + headers_FN)))\n",
    "print(\"Sensitivity, hit rate, recall, or true positive rate (TPR): {}\".format(headers_TP / (headers_TP + headers_FN)))\n",
    "print(\"Precision or positive predictive value (PPV): {}\".format(headers_TP / (headers_TP + headers_FP)))\n",
    "\n",
    "print('Ошибка первого рода (когда мы принимаем нормального пользователя за бота): {}'.format(headers_TN / y_test.shape[0]))\n",
    "print('Ошибка второго рода (когда мы принимаем бота за нормального пользователя): {}'.format(headers_FN / y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_parser.reassign_orders_values(values_data, order_data)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=list_ua)\n",
    "\n",
    "pairs_dict_list = []\n",
    "for row_idx in tqdm(range(len(order_data)), mininterval=2):\n",
    "    pairs_dict = {}\n",
    "    for first_p, second_p in combinations(order_data[row_idx], 2):\n",
    "        if order_data[row_idx][first_p] < order_data[row_idx][second_p]:\n",
    "            pairs_dict['{0} < {1}'.format(first_p, second_p)] = 1\n",
    "        else:\n",
    "            pairs_dict['{0} < {1}'.format(second_p, first_p)] = 1\n",
    "    pairs_dict_list.append(pairs_dict)\n",
    "    \n",
    "pca.fit(pairs_dict_list)\n",
    "pairs_dict_list = pca.transform(pairs_dict_list)\n",
    "\n",
    "sparse_dummy = orders_vectorizer.transform(pairs_dict_list).astype(np.int8)\n",
    "\n",
    "print('Sparse dummy orders shape: \\n{0}'.format(sparse_dummy.shape))\n",
    "\n",
    "trimmed_values_data = []\n",
    "\n",
    "for row_index in tqdm(range(len(values_data))):\n",
    "    tmp_row = {}\n",
    "    for key in important_keys_set:\n",
    "        if key in values_data[row_index]:\n",
    "            tmp_row[key] = values_data[row_index][key]\n",
    "    trimmed_values_data.append(tmp_row)\n",
    "\n",
    "sparse_dummy_values = values_vectorizer.transform(trimmed_values_data).astype(np.int8)\n",
    "\n",
    "print('Sparse dummy values shape: \\n{0}'.format(sparse_dummy_values.shape))\n",
    "\n",
    "full_sparce_dummy = hstack((sparse_dummy, sparse_dummy_values))\n",
    "full_sparce_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another User Agent representation\n",
    "\n",
    "### User Agent as tuple\n",
    "\n",
    "#### From Udger \n",
    "\n",
    "`UserAgent = {ua_family_code, ua_version, ua_class_code, device_class_code, os_family_code, os_code}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение\n",
    "\n",
    "Идея: Брать логи за последние 10 дней (нормально можно обучить) и замешивать в выборку юзерагенты из старых выборок которые входят в топ 200 старой выборки, но не входят в топ 200 новой\n",
    "\n",
    "Для выборки по времени у "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
