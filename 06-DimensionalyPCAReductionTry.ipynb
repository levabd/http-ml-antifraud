{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/levabd/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['plt']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 10)\n",
    "pd.set_option('display.max_columns', 1100)\n",
    "\n",
    "import os\n",
    "\n",
    "%pylab inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train part\n",
    "\n",
    "### Load data from logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from lib.parsers.logParser import LogParser\n",
    "\n",
    "l_parser = LogParser(log_folder='Logs/')\n",
    "\n",
    "main_data, values_data, order_data = l_parser.parse_train_sample(0, 1)\n",
    "\n",
    "list_ua = pd.DataFrame(main_data).User_Agent.value_counts().index.tolist()\n",
    "\n",
    "# For NaN Useragent\n",
    "list_ua.append('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 82984.45it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 533280.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse dummy orders shape: \n",
      "(20000, 1181)\n",
      "Sparse dummy values shape: \n",
      "(20000, 128)\n"
     ]
    }
   ],
   "source": [
    "important_keys_set = {'Accept', 'Accept-Charset', 'Accept-Encoding'}\n",
    "\n",
    "orders_vectorizer = sklearn.feature_extraction.DictVectorizer(sparse=True, dtype=float)\n",
    "values_vectorizer = sklearn.feature_extraction.DictVectorizer(sparse=True, dtype=float)\n",
    "\n",
    "l_parser.reassign_orders_values(order_data[:20000], values_data[:20000])\n",
    "full_sparce_dummy = l_parser.prepare_data(orders_vectorizer, values_vectorizer, important_keys_set, fit_dict=True)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "lb = preprocessing.LabelBinarizer(sparse_output=True)\n",
    "lb.fit(list_ua)\n",
    "y = lb.transform(pd.DataFrame(main_data[:20000]).User_Agent.fillna('0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 49s, sys: 480 ms, total: 13min 49s\n",
      "Wall time: 14min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "smart_clf = OneVsRestClassifier(LogisticRegression(C=100))\n",
    "smart_clf.fit(full_sparce_dummy, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with size(Bytes): 20977477\n",
      "Splitted in 0 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "filename = 'cls/dummyordr_and_3values_fullua_logreg_20K_cls.joblib.pkl'\n",
    "_ = joblib.dump(smart_clf, filename, compress=9)\n",
    "\n",
    "print(\"Model saved with size(Bytes): {}\".format(os.stat(filename).st_size))\n",
    "\n",
    "from lib.helpers.fileSplitter import split_file\n",
    "\n",
    "files_count = split_file(filename, 'parted-cls/dummyordr_and_3values_fullua_logreg_20K_cls.joblib.pkl')\n",
    "\n",
    "print('Splitted in {} files'.format(files_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test part\n",
    "\n",
    "### Prepare data (50/50 bots and human mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start parsing logs for distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start parsing logs for values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      "  0%|          | 131/75996 [00:00<00:57, 1308.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bots Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75996/75996 [00:48<00:00, 1575.30it/s]\n",
      "100%|██████████| 151992/151992 [00:01<00:00, 89733.78it/s]\n",
      " 41%|████▏     | 62982/151992 [00:00<00:00, 629811.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse dummy orders shape: \n",
      "(151992, 1181)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151992/151992 [00:00<00:00, 642704.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse dummy values shape: \n",
      "(151992, 128)\n"
     ]
    }
   ],
   "source": [
    "main_bot_data, values_bot_data, order_bot_data = l_parser.parse_bot_sample(10, 11, 20, 21)\n",
    "\n",
    "main_human_frame = pd.DataFrame(main_data[:20000])\n",
    "main_human_frame['is_human'] = True\n",
    "\n",
    "main_bot_frame = pd.DataFrame(main_bot_data[:20000])\n",
    "main_bot_frame['is_human'] = False\n",
    "\n",
    "main_all = pd.concat([main_human_frame, main_bot_frame])\n",
    "\n",
    "values_all = values_data[:20000] + values_bot_data[:20000]\n",
    "order_all = order_data[:20000] + order_bot_data[:20000]\n",
    "\n",
    "list_all_ua = main_all.User_Agent.value_counts().index.tolist()\n",
    "\n",
    "# For NaN Useragent\n",
    "list_all_ua.append('0')\n",
    "\n",
    "l_parser.reassign_orders_values(order_all, values_all)\n",
    "\n",
    "test_sparce_dummy = l_parser.prepare_data(orders_vectorizer, values_vectorizer, important_keys_set, fit_dict=False)\n",
    "\n",
    "lb.fit(list_all_ua)\n",
    "y_test = lb.transform(pd.DataFrame(main_all).User_Agent.fillna('0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load classifyer from file if needed\n",
    "\n",
    "Use only `dummyordr_and_3values_fulluacls.joblib.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infiles = [\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.0',\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.1',\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.2',\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.3',\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.4',\n",
    "    'parted-cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl.5'\n",
    "]\n",
    "\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "from lib.helpers.fileSplitter import cat_files\n",
    "\n",
    "cat_files(infiles, 'cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl')\n",
    "\n",
    "filename = 'cls/dummyordr_and_3values_fullua_logreg_cls.joblib.pkl'\n",
    "smart_clf = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "if (test_sparce_dummy.shape[0] * list_ua * 8 > mem.free) and memory_warn:\n",
    "    print(\"Not enought memory for predict proba calculation\")\n",
    "predictions_proba = smart_clf.predict_proba(test_sparce_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't calculate full sample. Because we hawven't enought memory.\n",
    "\n",
    "So we try to test top 20000 from each samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 85990.17it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 663317.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse dummy orders shape: \n",
      "(40000, 2277)\n",
      "Sparse dummy values shape: \n",
      "(40000, 361)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40000, 2638)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_all = pd.concat([main_human_frame, main_bot_frame]).head(20000)\n",
    "\n",
    "values_all = values_data[:20000] + values_bot_data[:20000]\n",
    "order_all = order_data[:20000] + order_bot_data[:20000]\n",
    "\n",
    "list_all_ua = main_all.User_Agent.value_counts().index.tolist()\n",
    "\n",
    "# For NaN Useragent\n",
    "list_all_ua.append('0')\n",
    "\n",
    "l_parser.reassign_orders_values(order_all, values_all)\n",
    "\n",
    "test_sparce_dummy = l_parser.prepare_data(orders_vectorizer, values_vectorizer, important_keys_set, fit_dict=False)\n",
    "\n",
    "lb.fit(list_all_ua)\n",
    "y_test = lb.transform(pd.DataFrame(main_all).User_Agent.fillna('0'))\n",
    "\n",
    "test_sparce_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictions proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 41s, sys: 9.1 s, total: 3min 50s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import psutil\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "if (test_sparce_dummy.shape[0] * len(list_ua) * 8 > mem.free) and memory_warn:\n",
    "    print(\"Not enought memory for predict proba calculation\")\n",
    "predictions_proba = smart_clf.predict_proba(test_sparce_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from lib.thresholdPredictions import ThresholdPredictions\n",
    "\n",
    "pred = ThresholdPredictions(user_agent_list=list_ua, clf=smart_clf)\n",
    "y_test_names, y_predicted, compare_answers, is_bot, answers_count = pred.bot_predict(lb, test_sparce_dummy, y_test, 0.024072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(compare_frame.iloc[0][0])\n",
    "print()\n",
    "for u_a in compare_frame.iloc[0][1]:\n",
    "    print(u_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_frame = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(y_test_names),\n",
    "        y_predicted, \n",
    "        pd.DataFrame(compare_answers), \n",
    "        main_all[:20000].is_human,\n",
    "        pd.DataFrame(is_bot), \n",
    "        pd.DataFrame(answers_count)\n",
    "    ], keys=['test', 'predicted', 'correctness', 'is_human', 'is_bot_predicted', 'count'], axis=1, join='inner')\n",
    "\n",
    "compare_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>predicted</th>\n",
       "      <th>correctness</th>\n",
       "      <th>is_human</th>\n",
       "      <th>is_bot_predicted</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>is_human</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...</td>\n",
       "      <td>[Mozilla/5.0 (compatible; YandexBot/3.0; +http...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mozilla/5.0 (compatible; bingbot/2.0; +http://...</td>\n",
       "      <td>[Mozilla/5.0 (compatible; YandexBot/3.0; +http...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.4.2; Zera S Buil...</td>\n",
       "      <td>[Mozilla/5.0 (compatible; YandexBot/3.0; +http...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) G...</td>\n",
       "      <td>[Mozilla/5.0 (compatible; YandexBot/3.0; +http...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mozilla/4.0 (compatible; MSIE 8.0; Windows NT ...</td>\n",
       "      <td>[Mozilla/5.0 (compatible; YandexBot/3.0; +http...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>Mozilla/5.0 (iPhone; CPU iPhone OS 10_2_1 like...</td>\n",
       "      <td>[Mozilla/5.0 (compatible; YandexBot/3.0; +http...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>Mozilla/5.0 (iPhone; CPU iPhone OS 10_2_1 like...</td>\n",
       "      <td>[Mozilla/5.0 (compatible; YandexBot/3.0; +http...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>Mozilla/5.0 (compatible; MJ12bot/v1.4.7; http:...</td>\n",
       "      <td>[Mozilla/5.0 (compatible; YandexBot/3.0; +http...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>Mozilla/5.0 (iPad; CPU OS 8_1 like Mac OS X) A...</td>\n",
       "      <td>[Mozilla/5.0 (compatible; YandexBot/3.0; +http...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>Mozilla/5.0 (Windows NT 5.1; rv:7.0.1) Gecko/2...</td>\n",
       "      <td>[Mozilla/5.0 (compatible; YandexBot/3.0; +http...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    test  \\\n",
       "                                                       0   \n",
       "0      Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...   \n",
       "1      Mozilla/5.0 (compatible; bingbot/2.0; +http://...   \n",
       "2      Mozilla/5.0 (Linux; Android 4.4.2; Zera S Buil...   \n",
       "3      Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) G...   \n",
       "4      Mozilla/4.0 (compatible; MSIE 8.0; Windows NT ...   \n",
       "...                                                  ...   \n",
       "19995  Mozilla/5.0 (iPhone; CPU iPhone OS 10_2_1 like...   \n",
       "19996  Mozilla/5.0 (iPhone; CPU iPhone OS 10_2_1 like...   \n",
       "19997  Mozilla/5.0 (compatible; MJ12bot/v1.4.7; http:...   \n",
       "19998  Mozilla/5.0 (iPad; CPU OS 8_1 like Mac OS X) A...   \n",
       "19999  Mozilla/5.0 (Windows NT 5.1; rv:7.0.1) Gecko/2...   \n",
       "\n",
       "                                               predicted correctness is_human  \\\n",
       "                                                       0           0 is_human   \n",
       "0      [Mozilla/5.0 (compatible; YandexBot/3.0; +http...        True     True   \n",
       "1      [Mozilla/5.0 (compatible; YandexBot/3.0; +http...        True     True   \n",
       "2      [Mozilla/5.0 (compatible; YandexBot/3.0; +http...        True     True   \n",
       "3      [Mozilla/5.0 (compatible; YandexBot/3.0; +http...        True     True   \n",
       "4      [Mozilla/5.0 (compatible; YandexBot/3.0; +http...        True     True   \n",
       "...                                                  ...         ...      ...   \n",
       "19995  [Mozilla/5.0 (compatible; YandexBot/3.0; +http...        True     True   \n",
       "19996  [Mozilla/5.0 (compatible; YandexBot/3.0; +http...        True     True   \n",
       "19997  [Mozilla/5.0 (compatible; YandexBot/3.0; +http...        True     True   \n",
       "19998  [Mozilla/5.0 (compatible; YandexBot/3.0; +http...        True     True   \n",
       "19999  [Mozilla/5.0 (compatible; YandexBot/3.0; +http...        True     True   \n",
       "\n",
       "      is_bot_predicted count  \n",
       "                     0     0  \n",
       "0                False  3304  \n",
       "1                False  3304  \n",
       "2                False  3304  \n",
       "3                False  3304  \n",
       "4                False  3304  \n",
       "...                ...   ...  \n",
       "19995            False  3304  \n",
       "19996            False  3304  \n",
       "19997            False  3304  \n",
       "19998            False  3304  \n",
       "19999            False  3304  \n",
       "\n",
       "[20000 rows x 6 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "headers_cm = confusion_matrix(~compare_frame.is_human, compare_frame.is_bot_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers_FP = headers_cm.sum(axis=0) - np.diag(headers_cm)  \n",
    "headers_FN = headers_cm.sum(axis=1) - np.diag(headers_cm)\n",
    "headers_TP = np.diag(headers_cm)\n",
    "headers_TN = headers_cm.sum() - (headers_FP + headers_FN + headers_TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: [15574     0]\n",
      "TN: [    0 15574]\n",
      "FP: [   0 4426]\n",
      "FN: [4426    0]\n",
      "Accuracy (ACC): [ 0.7787  0.7787]\n",
      "Sensitivity, hit rate, recall, or true positive rate (TPR): [ 0.7787     nan]\n",
      "Precision or positive predictive value (PPV): [ 1.  0.]\n",
      "Ошибка первого рода (когда мы принимаем нормального пользователя за бота): [ 0.          0.10246592]\n",
      "Ошибка второго рода (когда мы принимаем бота за нормального пользователя): [ 0.02911995  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('TP: {}'.format(headers_TP))\n",
    "print('TN: {}'.format(headers_TN))\n",
    "print(\"FP: {}\".format(headers_FP))\n",
    "print(\"FN: {}\".format(headers_FN))\n",
    "print(\"Accuracy (ACC): {}\".format((headers_TP + headers_TN) / (headers_TP + headers_TN + headers_FP + headers_FN)))\n",
    "print(\"Sensitivity, hit rate, recall, or true positive rate (TPR): {}\".format(headers_TP / (headers_TP + headers_FN)))\n",
    "print(\"Precision or positive predictive value (PPV): {}\".format(headers_TP / (headers_TP + headers_FP)))\n",
    "\n",
    "print('Ошибка первого рода (когда мы принимаем нормального пользователя за бота): {}'.format(headers_TN / y_test.shape[0]))\n",
    "print('Ошибка второго рода (когда мы принимаем бота за нормального пользователя): {}'.format(headers_FN / y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we see. This is wrong wrong wrong way. \n",
    "\n",
    "Even with 0 threshold we have only 50% True positive classification. It means that fulltext UserAgent \n",
    "\n",
    "### Soo we need аnother User Agent representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75996/75996 [00:00<00:00, 459741.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse dummy orders shape: \n",
      "(75996, 350)\n",
      "Sparse dummy values shape: \n",
      "(75996, 207)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-7e1c49851e74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sparse dummy values shape: \\n{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_dummy_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mfull_sparce_dummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_dummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_dummy_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mfull_sparce_dummy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levabd/anaconda3/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "pca.fit(orders_dummy)\n",
    "#sparse_dummy = scipy.sparse.csr_matrix(pca.transform(sparse_dummy))\n",
    "sparse_dummy = pca.transform(orders_dummy)\n",
    "\n",
    "print('Sparse dummy orders shape: \\n{0}'.format(sparse_dummy.shape))\n",
    "\n",
    "trimmed_values_data = []\n",
    "\n",
    "for row_index in tqdm(range(len(values_data))):\n",
    "    tmp_row = {}\n",
    "    for key in important_keys_set:\n",
    "        if key in values_data[row_index]:\n",
    "            tmp_row[key] = values_data[row_index][key]\n",
    "    trimmed_values_data.append(tmp_row)\n",
    "\n",
    "values_vectorizer.fit(trimmed_values_data)\n",
    "sparse_dummy_values = values_vectorizer.transform(trimmed_values_data).astype(np.int8)\n",
    "\n",
    "print('Sparse dummy values shape: \\n{0}'.format(sparse_dummy_values.shape))\n",
    "\n",
    "full_sparce_dummy = hstack((sparse_dummy, sparse_dummy_values))\n",
    "full_sparce_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00488503,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.00488503,  0.        , -0.01361608, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.00488503,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.01332112],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.00488503,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 29903/29903 [00:00<00:00, 70838.03it/s]\n"
     ]
    }
   ],
   "source": [
    "l_parser1 = LogParser(log_folder='Logs/')\n",
    "\n",
    "main_data1, values_data1, order_data1 = l_parser1.parse_train_sample(0, 1)\n",
    "\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "orders_vectorizer1 = sklearn.feature_extraction.DictVectorizer(sparse=True, dtype=float)\n",
    "values_vectorizer1 = sklearn.feature_extraction.DictVectorizer(sparse=True, dtype=float)\n",
    "pca1 = SparsePCA(n_components=350)\n",
    "\n",
    "pairs_dict_list1 = []\n",
    "for row_idx in tqdm(range(len(order_data1)), mininterval=2):\n",
    "    pairs_dict = {}\n",
    "    for first_p, second_p in combinations(order_data1[row_idx], 2):\n",
    "        if order_data1[row_idx][first_p] < order_data1[row_idx][second_p]:\n",
    "            pairs_dict['{0} < {1}'.format(first_p, second_p)] = 1\n",
    "        else:\n",
    "            pairs_dict['{0} < {1}'.format(second_p, first_p)] = 1\n",
    "    pairs_dict_list1.append(pairs_dict)\n",
    "\n",
    "orders_vectorizer1.fit(pairs_dict_list1)\n",
    "orders_dummy1 = orders_vectorizer1.transform(pairs_dict_list1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-994c7c3cb193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morders_dummy1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#sparse_dummy = scipy.sparse.csr_matrix(pca.transform(sparse_dummy))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msparse_dummy1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morders_dummy1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sparse dummy orders shape: \\n{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_dummy1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levabd/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/sparse_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m    110\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levabd/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 380\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levabd/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \"\"\"\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    244\u001b[0m                         \u001b[0;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "pca.fit(orders_dummy1)\n",
    "#sparse_dummy = scipy.sparse.csr_matrix(pca.transform(sparse_dummy))\n",
    "sparse_dummy1 = pca.transform(orders_dummy1)\n",
    "\n",
    "print('Sparse dummy orders shape: \\n{0}'.format(sparse_dummy1.shape))\n",
    "\n",
    "trimmed_values_data1 = []\n",
    "\n",
    "for row_index in tqdm(range(len(values_data1))):\n",
    "    tmp_row = {}\n",
    "    for key in important_keys_set:\n",
    "        if key in values_data1[row_index]:\n",
    "            tmp_row[key] = values_data1[row_index][key]\n",
    "    trimmed_values_data1.append(tmp_row)\n",
    "\n",
    "values_vectorizer1.fit(trimmed_values_data1)\n",
    "sparse_dummy_values1 = values_vectorizer1.transform(trimmed_values_data1).astype(np.int8)\n",
    "\n",
    "print('Sparse dummy values shape: \\n{0}'.format(sparse_dummy_values1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "full_sparce_dummy = hstack((sparse_dummy, sparse_dummy_values))\n",
    "full_sparce_dummy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
